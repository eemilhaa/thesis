% TODO add tech / software table?
\section{Materials and methods}

\subsection{Study design}

% Brief overview
At a general level the study constituted two main parts:
the development of the map presentation and
the survey to find out how
people use and understand the presentation.
While the primary goal of the development process was
to produce the map presentation and enable the survey,
the development process in and of itself was crucial to the study as well.
Its purpose was to be the framework that
allows me to test and answer my research questions
about the more technology-centric aspects of
the making of an interactive map.

So, a large part of this study was
in essence a software development project,
and the parts that were not, were still reliant on
the software development project succeeding.
This kind of setting,
combined with the author's limited experience with the relevant technologies,
inherently introduced a high level of risk and uncertainty to the study.
To minimize these factors,
I heavily utilized modern software development methodologies
\parencite{saq2020, bec2001, sha2017} in planning the development process
and designing the study.
Based on these,
I formatted the following points of focus
to guide the development process:
\begin{itemize}
	\item Plan minimally and adapt the plan constantly.
	\item Prioritize a working state of the entire system over details in single components.
	\item Adapt to technical constraints at the start, not at the end.
\end{itemize}

By adhering to these principles I could at an early stage see whether the system --
and, by extension, the entire study -- is realistic.
It also made it possible for me to avoid wasted effort and, most importantly,
make sure I am developing a system that is valuable to the entire study.
Iteratively improving the system as a functional whole
allowed me to consider the map interface
from the perspective of the map user as early as possible,
better integrating the survey into the study.
This is important as the map presentation is simultaneously an output of the development process
and an input to the design of the survey.

For an overview of the study design see figure \ref{fig:study design}.
% This allowed me to improve the relevancy of my research. TODO Discussion?
% I say this not only in the context of gaining valuable results to my research questions --
% I want to emphasize that to even know what research questions to ask is impossible with a linear approach.

% While the development process and the survey were linked to each other,
% they each had their separate goals and outputs too.
% In addition to producing the map presentation,
% the development process had to enable testing and answering my research questions
% about the making of an interactive map.
% This placed increased
% The goal of the survey is to gain insight on
% how the interactive map works as a representation of the mapped phenomenon.
% With the survey I collect data on map usage,
% which I in turn analyse to answer my research questions related to the map usage.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{visual/figures/diagrams/study_design.png}
	\caption{An overview of the study design}
	\label{fig:study design}
\end{figure}


\subsection{Data -- Helsinki region Travel Time Matrix}

The \acrlong{ttm} (\acrshort{ttm}) \parencite{fin2023}
is a dataset containing information of travel times and distances
in the Helsinki region in southern Finland.
This dataset was crucial to developing the map application,
as it was the sole source of the travel times shown on the map.
The dataset and the set of methods with which it is produced are open-source.

% Describe ttm in general: ykr, origin dest pairs etc (more surface level stuff common to all matrices)
A significant component of the \acrshort{ttm} is the \acrlong{ykr} (\acrshort{ykr})
statistical grid made by the Finnish Environmental Institute.
The grid has a spatial resolution of 250x250m, and it covers the entire Finland.
Most importantly, however, the part of the \acrshort{ykr} grid that overlaps with
the Helsinki region provides the spatial component for
the travel times stored in the \acrshort{ttm}.
The spatial extent of the dataset is shown in figure \ref{fig:ttm extent}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{visual/figures/ttm/ttm_extent.pdf}
	\caption{The location and extent of the TTM}
	\label{fig:ttm extent}
\end{figure}

The \acrshort{ttm} stores the travel times and distances
from every \acrshort{ykr} grid cell to every other  \acrshort{ykr} grid cell
within the Helsinki region.
The Helsinki region fits 13231 \acrshort{ykr} grid cells,
which means that the complete \acrshort{ttm} contains travel times and distances for
over 175 million routes.

All the routes are calculated for multiple different travel modes.
The primary travel modes are walking, cycling, public transportation and private car,
and, when applicable, each mode has variations based on
time of day and / or walking or cycling speed.
Time of day is especially relevant to motorized transport
in the form of the rush hour, for example,
while walking speed affects any travel mode
where a significant portion of the trip is covered on foot.
% Modelling the travel times accounts for walking,
% transfer times based on public transit schedules,
% and the actual geometries of the routes.

% TODO talk briefly about methodology, open source

% Describe the particular matrix (2023), methodology etc
\acrshort{ttm}s have been calculated for multiple years.
Differences between these datasets exist in the applied methodologies and thus the content
\parencite{ten2020}.
Also, the underlying city structure is by no means static over the years
which further introduces differences.
The map presentation I developed in this study
is based on the 2023 version of the \acrshort{ttm},
the newest one at the time of writing.
See table \ref{tab:ttm description} for an overview of the dataset,
and table \ref{tab:ttm table structure} for all the travel modes in the dataset.

\input{visual/tables/ttm_description.tex}

\input{visual/tables/ttm_table_structure.tex}



\subsection{Implementation of the map presentation}

\subsubsection{Software requirements}
% The requirements of the system are an essential aspect to consider,
% since they
% For example, the sheer scale and detail of the dataset being visualized
% means that instantaneous interaction with the map is not realistic
% if no detail of the mapped data is to be sacrificed.
% These kinds of tradeoffs are important to recognize,
% because only through them is it possible to specify what
% requirements should, or even could, be placed on the map application.

% TODO copypasta

% Roughly what kind of a map? Why?
% Based on the background study,  % TODO
Software requirements are functionalities and properties
that a given system should have \parencite{chu2009}.
They are an essential aspect to consider in all software development
as they dictate much of how any implementation is carried out \parencite{saq2020}.
Software requirements are often divided into
functional and nonfunctional requirements.
Functional requirements define the user-facing features of the system
while nonfunctional requirements describe the properties of a system
\parencite{chu2009}.
Nonfunctional requirements can be further divided into
quality attributes and constraints.
Quality attributes describe \textit{how} the software should be,
while constraints mean technical limitations \parencite{chu2009}.

I based the requirements of this particular system on the goals of the study,
considering the interactive map presentation.
The starting point for specifying these requirements
was my decision to prioritize real-time interaction over minute detail in the map,
the goal of the map interface being to act as a dynamic overview to the entire \acrshort{ttm}.
As mentioned in the previous section,
the spatial dimension of the \acrshort{ttm} is large.
If all of it is to be explored,
selecting different locations must be effortless and instantaneous.
The other dimension, travel mode, is much smaller,
but it should also be interactively selectable.
These two goals, while still quite vague,
immediately placed a number of requirements on the system.
From the perspective of the user,
there must be functionalities for interacting with the map to
select different locations, preferably very rapidly,
and different travel modes.
A significant quality attribute to consider is that the system should
be responsive enough for instantaneous real-time interactivity.
At the very start I also made the decision to target the web as the platform.
This constraint was the starting point for considering the technologies
for carrying out the implementation.
% To achieve this, the latency between user input and
% its visible results must be minimized.

In accordance with the guideline of continuous adaptation
\parencite{bec2001} I specified the software requirements loosely at first,
further focusing them as the system took shape.
To communicate the development process in an orderly manner,
and to motivate the design decisions I've made, I present
the requirements of the final system here.
Still, I should emphasize that the reality is not this linear.

The functional requirements (table \ref{tab:functional requirements})
comprise two modes of interaction for selecting locations:
clicking the map with a pointing device to produce a static map of the clicked location,
and continuously changing the location by moving the mouse over the map
(referred to as hovering).
I also specify that interface capabilities for selecting travel modes
and changing the mode of location selection should be present.
For the quality attributes of the system (table \ref{tab:quality attributes})
performance is an essential consideration,
both in the sense of software performance
and the responsiveness of the application as perceived by the user.
This must be considered in both the front- and backend of the system.
Other important aspects are that the system can be maintained and deployed,
and that it can meet different real-world usage loads --
if these attributes were lacking,
the work I've carried out would have little real value,
as it would be left available only to me.
As mentioned earlier,
the application is constrained to running in a web browser (table \ref{tab:constraints}).
Also -- as a general best-practice, as well as a constraint
necessitated by the available deployment environment --  % ref csc?
the implementation must be completely containerized (table \ref{tab:constraints}).

\input{visual/tables/functional_requirements.tex}
\input{visual/tables/nonfunctional_requirements}

\subsubsection{Assessing data preprocessing approaches}
% The need for preprocessing
% - what would raw data be like?
% - why as much as possible should be precalculated
% && the requirements preprocessing must satisfy
Preprocessing and simplifying the \acrshort{ttm} was a significant part of the study.
Soon after starting the development,
it became obvious that using unprocessed \acrshort{ttm} data was not an option
due to the unresponsiveness of rendering and the large file sizes.
Because of this, in the initial testing phase of the development
I formed the hypothesis that either file sizes
or the geometrical complexity of the data would be
the major factor limiting the responsiveness of the map,
i.e. the performance bottleneck.

So it was clear that some type and level of simplification of the 
data was necessary to enable real-time interaction in the map.
The goal of my assessment of different approaches to data preprocessing, thus,
was to find the suitable level and types of simplification,
and to compare the effectiveness of different approaches.

When assessing the preprocessing approaches, I focused on:
\begin{enumerate}
	\item The impact on file size, and thus on the speed of transferring data over internet.
	\item The impact on rendering speed of the map when mapping the data.
	\item The impact on the visual detail of data,
	i.e. how much information is lost.
\end{enumerate}

I assessed four preprocessing approaches:
\begin{enumerate}
	\item Aggregation of travel time data into isochrones (figure \ref{fig:isochrones})
	\item Limiting the maximum travel time, i.e. the geographical extent of the results (no figure yet)
	\item Reducing geometry precision
	\item File structure optimizations (simplifying GeoJSON structure, compression using gzip)
\end{enumerate}

To carry out the assessment
I constructed a modular Python-based preprocessing pipeline
(figure \ref{fig:preprocessing}).
The idea was that the complete pipeline applies all preprocessing approaches,
but the modular design allows for
isolated testing of the
different components of the pipeline, with different parameters.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{visual/figures/diagrams/preprocessing.png}
	\caption{Preprocessing}
	\label{fig:preprocessing}
\end{figure}

File sizes were the simplest factor to test for.
I tested each approach on
a randomly picked set of 100 locations in the \acrshort{ttm},
for each travel mode.
I picked a random sample of locations since the
data is of a different level of complexity based on location.
To measure the effect on file size I then calculated the mean file size it takes to represent
all the travel modes for a single cell in the dataset
when a given approach has been applied.

SIDENOTE: Filesize tests are in a notebook here: https://github.com/eemilhaa/travel-time-matrix-visualisation-preprocessing/blob/cleanup/example.ipynb.

I assessed the impact the preprocessing approaches have on rendering speed
with the finished map application.
The details of the map application are in the next section.
The assessment was qualitative,
as it was based on my own perception of responsiveness when using the map.
Quantitative testing of rendering speed would have been preferable,
but it proved to be difficult.
While tools for profiling rendering speed exist,
I did not succeed in two main areas
that would have been necessary for representative results.
I did not manage to limit the timing to strictly the map
component of the interface in such a way that
the test results would be reproducible.
I also did not find a way to automate a set of renders
on a sample of locations.
Averaging the renders of a single location would not suffice
for reasons stated above.
The tools I tried were the Firefox profiler
and the React developer tools browser extension on Firefox and Chromium
(I'll need to properly refer to these).

I carried out the assessment of the impact on visual detail of the data in two ways:
Qualitatively for all approaches by observing the simplified data on the map,
and, in the case of limiting maximum travel time,
quantitatively by calculating the percentage of the total dataset area that the simplified data covers.
I calculated these values as averages of a random sample of 100 locations, for each travel mode.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{visual/figures/ttm/isochrone_interval_10.pdf}
		\caption{Isochrones with an interval of 10 minutes}
		\label{fig:interval 10}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{visual/figures/ttm/isochrone_interval_20.pdf}
		\caption{Isochrones with an interval of 20 minutes}
		\label{fig:interval 20}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{visual/figures/ttm/isochrone_interval_30.pdf}
		\caption{Isochrones with an interval of 30 minutes}
		\label{fig:interval 30}
	\end{subfigure}%
	\caption{Travel time to Helsinki railway station by public transport mapped with isochrones of different intervals}
	\label{fig:isochrones}
\end{figure}

SIDENOTE: I intend to add similar figures of limiting maximum travel time here.

\subsubsection{Assessing web mapping libraries}

The solution for rendering the data to a map, i.e. the web mapping library,
was one of the most important decisions of the development process. 
In addition to rendering data correctly and efficiently,
the mapping library must allow for user interaction not only with the map,
but the underlying application.
Often, interaction with a web map changes only the state of the map,
for example by zooming,
and the web-mapping library handles an internal state change like this automatically.
With the map presentation I developed,
the map interaction must also change state outside the map.
For example:
Location selection should result in the fetching of new data from the backend,
and the mode of map interaction must be changed by map interaction.

Controlling state in web-applications is often best done by a specified solution,
such as a state store or, if applicable, a UI framework.
For this map presentation I chose to control application state with React,
as it is the most widely used and documented UI framework,
and also the only UI framework that many web-mapping libraries have integration capabilities for.
In general, integration with a framework instead of a self-made solution
means less room for error in the implementation,
and better maintainability and extensibility as the amount of code is drastically smaller.
Also, I used React in implementing the few non-map user-interface elements
(travel mode selection, legend, tooltip),
which meant no additional dependencies.

I based my assessment of web mapping libraries on three criteria:
\begin{enumerate}
	\item Visual quality of the map
	\item Responsiveness of the map
	\item Integration with React
\end{enumerate}

SIDENOTE: I don't know if I should be so technology-specific with React.
React really isn't the point, the point is that a UI framework is sensible
to not have to do everything from the ground up. React just happens to
be \textit{the} framework
(and the only framework pretty much any mapping library integrates with in any way).

When selecting which web mapping libraries to assess, I used the following properties as a baseline:
\begin{enumerate}
	\item Free and open source licensing
	\item Actively maintained
	\item At least some level of integration with React
\end{enumerate}

With these considerations I arrived at three potential libraries:
Leaflet, Maplibre and Deck.gl.
To assess the criteria above,
I used each library as the map interface with the application.
I assessed visual quality of the map qualitatively, and, for the reasons I stated in the previous chapter,
I had to resort to qualitative assessment for rendering performance as well.
For assessing integration with react additionally I utilized online documentation of potential solutions.

% % TODO copypasta
% Translucent Overlay (OV) is the best technique over-
% all, which makes it a good choice when only one comparison
% technique should be provided to user \parencite{lob2015}.


\subsubsection{Technical architecture}

\begin{itemize}
	\item Backend: Minimal HTTP sever serving pre-compressed geojson files directly from filesystem
	\item Scalable Kubernetes / OpenShift deployments → Reproducibility at the level of the entire system
	\item Containerization → Reproducibility at the level of the components of the system
	\item Modularity of the design
	\item the list goes on, I'll have to consider what matters and what doesn't.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{visual/figures/diagrams/architechture.png}
	\caption{Architecture}
	\label{fig:architechture}
\end{figure}

SIDENOTE: I'll add figures of the components (at least the map application) here as suggested by Pyry.


% The need for serving (backend)
% - why data must be decoupled
% && the requirements serving must satisfy

% Different approaches

% Why nginx + static files?

% Describe how it was done

\subsection{Survey}

\subsubsection{Questionnaire design and structure}

The questionnaire is structured to:
\begin{enumerate}
	\item Prompt the participant to use the map for different tasks
	\item Ask questions about the participant's experience
	on using the map for completing the tasks
\end{enumerate}

Reasoning about the design: online questionnaire, tasks composed of prompts and questions.
Descriptions of what the purpose of each task was.

\subsubsection{Survey distribution and participants}
How the survey was distributed,
Descrtiption of the sample.

(Response plots in appendices)
